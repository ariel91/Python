{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networked programs\n",
    "\n",
    "Si bien muchos de los ejemplos de este libro se han centrado en leer archivos y buscar datos en esos archivos, hay muchas fuentes diferentes de información cuando uno considera Internet.\n",
    "\n",
    "En este capítulo pretendemos ser un navegador web y recuperar páginas web utilizando el Protocolo de transferencia de hipertexto (HTTP). Luego leeremos los datos de la página web y lo analizaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperText Transfer Protocol - HTTP\n",
    "El protocolo de red que da poder a la web es en realidad bastante simple y hay soporte incorporado en los <b>sockets</b> llamados Python que hace que sea muy fácil hacer conexiones de red y recuperar datos sobre esos sockets en un programa de Python.\n",
    "\n",
    "Un socket es muy similar a un archivo, excepto que un solo socket proporciona una conexión bidireccional entre dos programas. Puede leer y escribir en el mismo socket. Si escribe algo en un socket, se envía a la aplicación en el otro extremo del socket. Si lee desde el socket, se le proporcionan los datos que la otra aplicación ha enviado.\n",
    "\n",
    "Pero si intenta leer un socket cuando el programa en el otro extremo del socket no ha enviado ningún dato, simplemente siéntese y espere. Si los programas en ambos extremos del socket simplemente esperan algunos datos sin enviar nada, esperarán durante mucho tiempo.\n",
    "\n",
    "Entonces, una parte importante de los programas que se comunican a través de Internet es tener algún tipo de protocolo. Un protocolo es un conjunto de reglas precisas que determinan quién debe ir primero, qué deben hacer y, a continuación, cuáles son las respuestas a ese mensaje y quién envía a continuación, y así sucesivamente. En cierto sentido, las dos aplicaciones en cada extremo del zócalo están haciendo un baile y asegurándose de no pisar los dedos del otro.\n",
    "\n",
    "Hay muchos documentos que describen estos protocolos de red. El protocolo de transferencia de hipertexto se describe en el siguiente documento:\n",
    "\n",
    "http://www.w3.org/Protocols/rfc2616/rfc2616.txt\n",
    "\n",
    "\n",
    "Este es un documento largo y complejo de 176 páginas con muchos detalles. Si le parece interesante, siéntase libre de leerlo todo. Pero si echas un vistazo a la página 36 de RFC2616, encontrarás la sintaxis para la solicitud GET. Para solicitar un documento de un servidor web, hacemos una conexión al servidor www.pr4e.org en el puerto 80 y luego enviamos una línea del formulario\n",
    "\n",
    "GET http://data.pr4e.org/romeo.txt HTTP / 1.0\n",
    "\n",
    "donde el segundo parámetro es la página web que estamos solicitando, y luego también enviamos una línea en blanco. El servidor web responderá con cierta información de encabezado sobre el documento y una línea en blanco seguida del contenido del documento.\n",
    "\n",
    "## The World's Simplest Web Browser\n",
    "Quizás la forma más fácil de mostrar cómo funciona el protocolo HTTP es escribir un programa Python muy simple que hace una conexión a un servidor web y sigue las reglas del protocolo HTTP para solicitar un documento y mostrar lo que el servidor envía de vuelta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Sat, 12 May 2018 07:01:58 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\r\n",
      "ETag: \"a7-54f6609245537\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 167\r\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\r\n",
      "Pragma: no-cache\r\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(20)\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "mysock.close()\n",
    "\n",
    "# Code: http://www.py4e.com/code3/socket1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, el programa establece una conexión con el puerto 80 en el servidor www.py4e.com. Como nuestro programa cumple la función de \"navegador web\", el protocolo HTTP dice que debemos enviar el comando GET seguido de una línea en blanco\n",
    "![image.png](socket.png) <b>mysock.connect(('data.pr4e.org', 80))</b> ![image.png](datapy4e.png) \n",
    "\n",
    "## A Socket Connection\n",
    "\n",
    "Una vez que enviamos esa línea en blanco, escribimos un bucle que recibe datos en trozos de 512 caracteres del socket e imprime los datos hasta que no haya más datos para leer (es decir, el recv () devuelve una cadena vacía).\n",
    "\n",
    "El programa produce el resultado <b>ya mostrado</b>.\n",
    "\n",
    "<b>Explicacion de la salida</b> </n>\n",
    "La salida comienza con los encabezados que envía el servidor web para describir el documento. Por ejemplo, el encabezado Content-Type indica que el documento es un documento de texto plano (texto / plano).\n",
    "\n",
    "Después de que el servidor nos envíe los encabezados, agrega una línea en blanco para indicar el final de los encabezados, y luego envía los datos reales del archivo romeo.txt.\n",
    "\n",
    "<b>Este ejemplo muestra cómo hacer una conexión de red de bajo nivel con sockets</b>. Los sockets se pueden usar para comunicarse con un servidor web o con un servidor de correo u otros tipos de servidores. Todo lo que se necesita es encontrar el documento que describe el protocolo y escribir el código para enviar y recibir los datos de acuerdo con el protocolo.\n",
    "\n",
    "Sin embargo, dado que el protocolo que utilizamos más comúnmente es el protocolo web HTTP, Python tiene una biblioteca especial diseñada específicamente para admitir el protocolo HTTP para la recuperación de documentos y datos en la web.\n",
    "\n",
    "## Retrieving an image over HTTP (recuperando una imagen http)\n",
    "\n",
    "En el ejemplo anterior, recuperamos un archivo de texto sin formato que tenía nuevas líneas en el archivo y simplemente copiamos los datos en la pantalla mientras se ejecutaba el programa. Podemos usar un programa similar para recuperar una imagen usando HTTP. En lugar de copiar los datos en la pantalla mientras se ejecuta el programa, acumulamos los datos en una cadena, recortamos los encabezados y luego guardamos los datos de la imagen en un archivo de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120 5120\n",
      "5120 10240\n",
      "5120 15360\n",
      "5120 20480\n",
      "5120 25600\n",
      "5120 30720\n",
      "5120 35840\n",
      "5120 40960\n",
      "5120 46080\n",
      "5120 51200\n",
      "5120 56320\n",
      "5120 61440\n",
      "5120 66560\n",
      "5120 71680\n",
      "5120 76800\n",
      "5120 81920\n",
      "5120 87040\n",
      "5120 92160\n",
      "5120 97280\n",
      "5120 102400\n",
      "5120 107520\n",
      "5120 112640\n",
      "5120 117760\n",
      "5120 122880\n",
      "5120 128000\n",
      "5120 133120\n",
      "5120 138240\n",
      "5120 143360\n",
      "5120 148480\n",
      "5120 153600\n",
      "5120 158720\n",
      "5120 163840\n",
      "5120 168960\n",
      "5120 174080\n",
      "5120 179200\n",
      "5120 184320\n",
      "5120 189440\n",
      "5120 194560\n",
      "5120 199680\n",
      "5120 204800\n",
      "5120 209920\n",
      "5120 215040\n",
      "5120 220160\n",
      "5120 225280\n",
      "5120 230400\n",
      "208 230608\n",
      "Header length 394\n",
      "HTTP/1.1 200 OK\n",
      "Date: Sat, 12 May 2018 13:52:43 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Mon, 15 May 2017 12:27:40 GMT\n",
      "ETag: \"38342-54f8f2e5b6277\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 230210\n",
      "Vary: Accept-Encoding\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: image/jpeg\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import time\n",
    "\n",
    "HOST = 'data.pr4e.org'\n",
    "PORT = 80\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect((HOST, PORT))\n",
    "mysock.sendall(b'GET http://data.pr4e.org/cover3.jpg HTTP/1.0\\r\\n\\r\\n')\n",
    "count = 0\n",
    "picture = b\"\"\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(5120)\n",
    "    if (len(data) < 1): break\n",
    "    time.sleep(0.25)\n",
    "    count = count + len(data)\n",
    "    print(len(data), count)\n",
    "    picture = picture + data\n",
    "\n",
    "mysock.close()\n",
    "# Look for the end of the header (2 CRLF)\n",
    "pos = picture.find(b\"\\r\\n\\r\\n\")\n",
    "print('Header length', pos)\n",
    "print(picture[:pos].decode())\n",
    "\n",
    "# Skip past the header and save the picture data\n",
    "picture = picture[pos+4:]\n",
    "fhand = open(\"stuff4.jpg\", \"wb\")\n",
    "fhand.write(picture)\n",
    "fhand.close()\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urljpeg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ver que para esta url, el encabezado Content-Type indica que el cuerpo del documento es una imagen (image / jpeg). Una vez que el programa finalice, puede ver los datos de la imagen abriendo el archivo stuff.jpg en un visor de imágenes.\n",
    "\n",
    "A medida que se ejecuta el programa, puede ver que no obtenemos 5120 caracteres cada vez que llamamos al método recv (). Obtenemos todos los caracteres que el servidor web ha transferido a través de la red en el momento en que llamamos a recv (). En este ejemplo, obtenemos 1460 o 2920 caracteres cada vez que solicitamos hasta 5120 caracteres de datos.\n",
    "\n",
    "Sus resultados pueden ser diferentes dependiendo de la velocidad de su red. También tenga en cuenta que en la última llamada a recv () obtenemos 1681 bytes, que es el final de la secuencia, y en la siguiente llamada a recv () obtenemos una cadena de longitud cero que nos dice que el servidor ha llamado close ( ) en su extremo del zócalo y no hay más información próxima.\n",
    "\n",
    "Podemos ralentizar nuestras llamadas recv () sucesivas al quitar la llamada a time.sleep (). De esta manera, esperamos un cuarto de segundo después de cada llamada para que el servidor pueda \"adelantarse\" y enviarnos más datos antes de volver a llamar a recv (). Con el retraso, el programa se ejecuta de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120 5120\n",
      "2140 7260\n",
      "1452 8712\n",
      "1452 10164\n",
      "1452 11616\n",
      "5120 16736\n",
      "688 17424\n",
      "4356 21780\n",
      "5120 26900\n",
      "2140 29040\n",
      "5120 34160\n",
      "672 34832\n",
      "5120 39952\n",
      "2156 42108\n",
      "1452 43560\n",
      "5120 48680\n",
      "656 49336\n",
      "5120 54456\n",
      "704 55160\n",
      "1452 56612\n",
      "1468 58080\n",
      "1452 59532\n",
      "1452 60984\n",
      "5120 66104\n",
      "688 66792\n",
      "1452 68244\n",
      "1452 69696\n",
      "2904 72600\n",
      "5120 77720\n",
      "688 78408\n",
      "1452 79860\n",
      "1452 81312\n",
      "5120 86432\n",
      "688 87120\n",
      "5120 92240\n",
      "2140 94380\n",
      "5120 99500\n",
      "688 100188\n",
      "5120 105308\n",
      "2140 107448\n",
      "1452 108900\n",
      "1452 110352\n",
      "1452 111804\n",
      "1452 113256\n",
      "5120 118376\n",
      "2140 120516\n",
      "1452 121968\n",
      "1452 123420\n",
      "1452 124872\n",
      "1452 126324\n",
      "4356 130680\n",
      "5120 135800\n",
      "688 136488\n",
      "5120 141608\n",
      "688 142296\n",
      "2904 145200\n",
      "5120 150320\n",
      "688 151008\n",
      "5120 156128\n",
      "688 156816\n",
      "2904 159720\n",
      "5120 164840\n",
      "688 165528\n",
      "5120 170648\n",
      "688 171336\n",
      "2904 174240\n",
      "5120 179360\n",
      "688 180048\n",
      "5120 185168\n",
      "688 185856\n",
      "2904 188760\n",
      "5120 193880\n",
      "688 194568\n",
      "5120 199688\n",
      "688 200376\n",
      "5120 205496\n",
      "688 206184\n",
      "5120 211304\n",
      "688 211992\n",
      "5120 217112\n",
      "688 217800\n",
      "5120 222920\n",
      "2140 225060\n",
      "5120 230180\n",
      "428 230608\n",
      "Header length 394\n",
      "HTTP/1.1 200 OK\n",
      "Date: Sat, 12 May 2018 13:59:20 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Mon, 15 May 2017 12:27:40 GMT\n",
      "ETag: \"38342-54f8f2e5b6277\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 230210\n",
      "Vary: Accept-Encoding\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: image/jpeg\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import time\n",
    "\n",
    "HOST = 'data.pr4e.org'\n",
    "PORT = 80\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect((HOST, PORT))\n",
    "mysock.sendall(b'GET http://data.pr4e.org/cover3.jpg HTTP/1.0\\r\\n\\r\\n')\n",
    "count = 0\n",
    "picture = b\"\"\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(5120)\n",
    "    if (len(data) < 1): break\n",
    "    count = count + len(data)\n",
    "    print(len(data), count)\n",
    "    picture = picture + data\n",
    "\n",
    "mysock.close()\n",
    "# Look for the end of the header (2 CRLF)\n",
    "pos = picture.find(b\"\\r\\n\\r\\n\")\n",
    "print('Header length', pos)\n",
    "print(picture[:pos].decode())\n",
    "\n",
    "# Skip past the header and save the picture data\n",
    "picture = picture[pos+4:]\n",
    "fhand = open(\"stuff4.jpg\", \"wb\")\n",
    "fhand.write(picture)\n",
    "fhand.close()\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urljpeg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, aparte de la primera y la última llamada a recv (), ahora obtenemos 5120 caracteres cada vez que solicitamos nuevos datos.\n",
    "\n",
    "Hay un búfer entre el servidor que realiza las solicitudes send () y nuestra aplicación que realiza solicitudes recv (). Cuando ejecutamos el programa con la demora en su lugar, en algún momento el servidor puede llenar el búfer en el zócalo y verse obligado a pausar hasta que nuestro programa comience a vaciar el búfer. La pausa de la aplicación de envío o la aplicación de recepción se denomina <b>\"control de flujo\"</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML using BeautifulSoup\n",
    "\n",
    "Hay varias bibliotecas de Python que pueden ayudarlo a analizar HTML y extraer datos de las páginas. Cada una de las bibliotecas tiene sus fortalezas y debilidades y puede elegir una según sus necesidades.\n",
    "\n",
    "Como ejemplo, simplemente analizaremos algunos enlaces de entrada y extracción de HTML utilizando la biblioteca BeautifulSoup. Puede descargar e instalar el código BeautifulSoup desde:\n",
    "\n",
    "http://www.crummy.com/software/\n",
    "\n",
    "Puede descargar e \"instalar\" BeautifulSoup o simplemente puede colocar el archivo BeautifulSoup.py en la misma carpeta que su aplicación.\n",
    "\n",
    "Aunque el HTML se parece a XML1i y algunas páginas están cuidadosamente construidas para ser XML, la mayoría de los HTML se suelen romper de tal forma que hacen que un analizador XML rechace toda la página de HTML como inadecuada. BeautifulSoup tolera HTML altamente defectuoso y aún le permite extraer fácilmente los datos que necesita.\n",
    "\n",
    "Usaremos urllib para leer la página y luego usaremos BeautifulSoup para extraer los atributos de href de las etiquetas de anclaje (a).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - https://www.dr-chuck.com/\n",
      "https://www.dr-chuck.com/csev-blog/\n",
      "https://www.si.umich.edu/\n",
      "https://www.ratemyprofessors.com/ShowRatings.jsp?tid=1159280\n",
      "https://www.dr-chuck.com/csev-blog/\n",
      "https://www.twitter.com/drchuck/\n",
      "https://www.dr-chuck.com/dr-chuck/resume/speaking.htm\n",
      "https://www.slideshare.net/csev\n",
      "/dr-chuck/resume/index.htm\n",
      "https://amzn.to/1K5Q81K\n",
      "http://afs.dr-chuck.com/papers/\n",
      "https://itunes.apple.com/us/podcast/computing-conversations/id731495760\n",
      "https://www.youtube.com/playlist?list=PLHJB2bhmgB7dFuY7HmrXLj5BmHGKTD-3R\n",
      "https://developers.imsglobal.org/\n",
      "https://www.youtube.com/user/csev\n",
      "https://vimeo.com/drchuck/videos\n",
      "https://backpack.openbadges.org/share/4f76699ddb399d162a00b89a452074b3/\n",
      "https://www.linkedin.com/in/charlesseverance/\n",
      "https://www.researchgate.net/profile/Charles_Severance/\n",
      "https://www.tsugicloud.org/\n",
      "/office\n",
      "https://www.coursera.org/course/pythonlearn\n",
      "https://www.coursera.org/course/insidetheinternet\n",
      "https://open.umich.edu/education/si/si502/winter2009/\n",
      "http://www.pythonlearn.com\n",
      "http://www.php-intro.com/\n",
      "http://www.appenginelearn.com/\n",
      "http://www.pythonlearn.com/\n",
      "/sakai-book\n",
      "http://www.amazon.com/gp/product/1624311393/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=1624311393&linkCode=as2&tag=drchu02-20\n",
      "http://www.amazon.com/gp/product/059680069X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=059680069X&linkCode=as2&tag=drchu02-20\n",
      "http://www.amazon.com/Performance-Computing-Architectures-Optimization-Benchmarks/dp/156592312X/\n",
      "http://oreilly.com/catalog/9781565923126/\n",
      "http://cnx.org/content/col11136/latest/\n",
      "http://www.youtube.com/playlist?list=PLHJB2bhmgB7dFuY7HmrXLj5BmHGKTD-3R\n",
      "https://www.vimeo.com/17207620\n",
      "https://www.youtube.com/watch?v=BVKpW02hsrU\n",
      "https://www.youtube.com/watch?v=sa2WsgCvn7c\n",
      "https://www.vimeo.com/17213019\n",
      "https://www.youtube.com/watch?v=FJ078sO35M0\n",
      "http://afs.dr-chuck.com/citoolkit\n",
      "https://www.sakaiproject.org/\n",
      "https://www.tsugi.org/\n",
      "https://developers.imsglobal.org/\n",
      "/obi-sample\n",
      "https://twitter.com/drchuck\n"
     ]
    }
   ],
   "source": [
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')# Solicitud de la direccion web\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser') # El analizador de beautiful inicia a procesar\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urllinks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El programa solicita una dirección web, luego abre la página web, lee los datos y los pasa al analizador BeautifulSoup, y luego recupera todas las etiquetas de anclaje e imprime el atributo href para cada etiqueta.\n",
    "\n",
    "Cuando el programa se ejecuta, se ve de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://py4e-data.dr-chuck.net/known_by_Elivia.html\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Benjamyn.html\n",
      "Contents: Benjamyn\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Lilias.html\n",
      "Contents: Lilias\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Paislie.html\n",
      "Contents: Paislie\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Erica.html\n",
      "Contents: Erica\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Kalvin.html\n",
      "Contents: Kalvin\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Aamirah.html\n",
      "Contents: Aamirah\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Katarina.html\n",
      "Contents: Katarina\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Melandra.html\n",
      "Contents: Melandra\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Rhiannnon.html\n",
      "Contents: Rhiannnon\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Edwyn.html\n",
      "Contents: Edwyn\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Louanne.html\n",
      "Contents: Louanne\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Derryne.html\n",
      "Contents: Derryne\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Qandeel.html\n",
      "Contents: Qandeel\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Arwen.html\n",
      "Contents: Arwen\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Yu.html\n",
      "Contents: Yu\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Nirvana.html\n",
      "Contents: Nirvana\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Szymon.html\n",
      "Contents: Szymon\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Jayme.html\n",
      "Contents: Jayme\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Tokunbo.html\n",
      "Contents: Tokunbo\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Harley.html\n",
      "Contents: Harley\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Zohaib.html\n",
      "Contents: Zohaib\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Jillianne.html\n",
      "Contents: Jillianne\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Sinali.html\n",
      "Contents: Sinali\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Tadd.html\n",
      "Contents: Tadd\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Tarik.html\n",
      "Contents: Tarik\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Catriona.html\n",
      "Contents: Catriona\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Kaylea.html\n",
      "Contents: Kaylea\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Dennys.html\n",
      "Contents: Dennys\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Kyro.html\n",
      "Contents: Kyro\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Chukwuemeka.html\n",
      "Contents: Chukwuemeka\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Callum.html\n",
      "Contents: Callum\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Aleshia.html\n",
      "Contents: Aleshia\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Marion.html\n",
      "Contents: Marion\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Remo.html\n",
      "Contents: Remo\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Naima.html\n",
      "Contents: Naima\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Alhaji.html\n",
      "Contents: Alhaji\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Luic.html\n",
      "Contents: Luic\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Arved.html\n",
      "Contents: Arved\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Kuzivakwashe.html\n",
      "Contents: Kuzivakwashe\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Iniobong.html\n",
      "Contents: Iniobong\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Limo.html\n",
      "Contents: Limo\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Hazel.html\n",
      "Contents: Hazel\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Jade.html\n",
      "Contents: Jade\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Zendel.html\n",
      "Contents: Zendel\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Qainat.html\n",
      "Contents: Qainat\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Hadeel.html\n",
      "Contents: Hadeel\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Katherine.html\n",
      "Contents: Katherine\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Cleone.html\n",
      "Contents: Cleone\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Tessa.html\n",
      "Contents: Tessa\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Shanon.html\n",
      "Contents: Shanon\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Wendy.html\n",
      "Contents: Wendy\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Amberlouise.html\n",
      "Contents: Amberlouise\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Yolanda.html\n",
      "Contents: Yolanda\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Nathanial.html\n",
      "Contents: Nathanial\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Pari.html\n",
      "Contents: Pari\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Ifrah.html\n",
      "Contents: Ifrah\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Uchenna.html\n",
      "Contents: Uchenna\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Reynelle.html\n",
      "Contents: Reynelle\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Isabell.html\n",
      "Contents: Isabell\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Amira.html\n",
      "Contents: Amira\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Haleema.html\n",
      "Contents: Haleema\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Callyn.html\n",
      "Contents: Callyn\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Conley.html\n",
      "Contents: Conley\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Mikey.html\n",
      "Contents: Mikey\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Kieya.html\n",
      "Contents: Kieya\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Chimsom.html\n",
      "Contents: Chimsom\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Anees.html\n",
      "Contents: Anees\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Carlee.html\n",
      "Contents: Carlee\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Tammara.html\n",
      "Contents: Tammara\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Han.html\n",
      "Contents: Han\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Tailee.html\n",
      "Contents: Tailee\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Karine.html\n",
      "Contents: Karine\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Katrianne.html\n",
      "Contents: Katrianne\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Zane.html\n",
      "Contents: Zane\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Elisa.html\n",
      "Contents: Elisa\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Daryl.html\n",
      "Contents: Daryl\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Emile.html\n",
      "Contents: Emile\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Caydee.html\n",
      "Contents: Caydee\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Josi.html\n",
      "Contents: Josi\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Shayan.html\n",
      "Contents: Shayan\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Giacomo.html\n",
      "Contents: Giacomo\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Circe.html\n",
      "Contents: Circe\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Kylie.html\n",
      "Contents: Kylie\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Kiri.html\n",
      "Contents: Kiri\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Logyne.html\n",
      "Contents: Logyne\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Marwah.html\n",
      "Contents: Marwah\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Leigh.html\n",
      "Contents: Leigh\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Gurdeep.html\n",
      "Contents: Gurdeep\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Joan.html\n",
      "Contents: Joan\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Sukhi.html\n",
      "Contents: Sukhi\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Akira.html\n",
      "Contents: Akira\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Ikechukwu.html\n",
      "Contents: Ikechukwu\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Naoise.html\n",
      "Contents: Naoise\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Archie.html\n",
      "Contents: Archie\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Dionne.html\n",
      "Contents: Dionne\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Sayeed.html\n",
      "Contents: Sayeed\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Manus.html\n",
      "Contents: Manus\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Shalamar.html\n",
      "Contents: Shalamar\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Olaoluwapolorimi.html\n",
      "Contents: Olaoluwapolorimi\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Gabriel.html\n",
      "Contents: Gabriel\n"
     ]
    }
   ],
   "source": [
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')# Solicitud de la direccion web\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser') # El analizador de beautiful inicia a procesar\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    #print(tag.get('href', None))\n",
    "    #print('TAG:',tag)\n",
    "    print('URL:',tag.get('href', None))\n",
    "    print('Contents:',tag.contents[0])\n",
    "    #print ('Attrs:',tag.attrs)\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urllinks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://py4e-data.dr-chuck.net/comments_81301.html\n",
      "95\n",
      "188\n",
      "281\n",
      "374\n",
      "467\n",
      "558\n",
      "648\n",
      "733\n",
      "815\n",
      "895\n",
      "975\n",
      "1054\n",
      "1133\n",
      "1212\n",
      "1290\n",
      "1368\n",
      "1446\n",
      "1523\n",
      "1599\n",
      "1674\n",
      "1747\n",
      "1815\n",
      "1883\n",
      "1947\n",
      "2005\n",
      "2060\n",
      "2110\n",
      "2159\n",
      "2206\n",
      "2251\n",
      "2295\n",
      "2339\n",
      "2381\n",
      "2421\n",
      "2461\n",
      "2495\n",
      "2528\n",
      "2561\n",
      "2591\n",
      "2618\n",
      "2644\n",
      "2668\n",
      "2691\n",
      "2713\n",
      "2732\n",
      "2747\n",
      "2761\n",
      "2770\n",
      "2774\n",
      "2776\n",
      "Sum: 2776\n"
     ]
    }
   ],
   "source": [
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "a = 0\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')# Solicitud de la direccion web\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser') # El analizador de beautiful inicia a procesar\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "for tag in tags:\n",
    "    #print(tag.get('class', None))\n",
    "    #print('TAG:',tag)\n",
    "    #print('URL:',tag.get('class', None))\n",
    "    #a = int(tag.contents[0])\n",
    "    a = a + int(tag.contents[0])\n",
    "    print(a)\n",
    "    #print('Contents:',tag.contents[0])\n",
    "    #print ('Attrs:',tag.attrs)\n",
    "    \n",
    "print('Sum: '+'{}'.format(a))\n",
    "# Code: http://www.py4e.com/code3/urllinks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD:.ipynb_checkpoints/Networked programs-checkpoint.ipynb
     "ename": "AttributeError",
     "evalue": "module 'html5lib.treebuilders' has no attribute '_base'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b06c96200fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuilder_registry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdammit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnicodeDammit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m from .element import (\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/bs4/builder/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0mregister_treebuilders_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_htmlparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_html5lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0mregister_treebuilders_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_html5lib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/bs4/builder/_html5lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTreeBuilderForHtml5lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml5lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreebuilders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreeBuilder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespaceHTMLElements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'html5lib.treebuilders' has no attribute '_base'"
     ]
=======
     "data": {
      "text/plain": [
       "\"\\nimport urllib.request, urllib.parse, urllib.error\\nfrom bs4 import BeautifulSoup\\nimport ssl\\ncon = 0\\n# Ignore SSL certificate errors\\nctx = ssl.create_default_context()\\nctx.check_hostname = False\\nctx.verify_mode = ssl.CERT_NONE\\n\\nurl = input('Enter - ')# Solicitud de la direccion web\\nhtml = urllib.request.urlopen(url, context=ctx).read()\\nsoup = BeautifulSoup(html, 'html.parser') # El analizador de beautiful inicia a procesar\\n\\n# Retrieve all of the anchor tags\\ntags = soup('a')\\nfor tag in tags:\\n    con = con + 1\\n    c = con / 3\\n    if c == 1:\\n        print('URL:',tag.get('href', None))\\n        print('Contents:',tag.contents[0])\\n        print(con)\\n        url = tag.get('href', None)\\n        html = urllib.request.urlopen(url, context=ctx).read()\\n        soup = BeautifulSoup(html, 'html.parser')\\n        con = 0\\n        tags = soup('a')\\n        for tag2 in tags:\\n            con = con + 1\\n            c = con / 3\\n            if c == 1:\\n                print('URL:',tag2.get('href', None))\\n                print('Contents:',tag2.contents[0])\\n                print(con)\\n                url = tag2.get('href', None)\\n                html = urllib.request.urlopen(url, context=ctx).read()\\n                soup = BeautifulSoup(html, 'html.parser')\\n                con = 0\\n                tags = soup('a')\\n                for tag3 in tags:\\n                    con = con + 1\\n                    c = con / 3\\n                    if c == 1:\\n                        print('URL:',tag3.get('href', None))\\n                        print('Contents:',tag3.contents[0])\\n                        print(con)\\n                        url = tag3.get('href', None)\\n                        html = urllib.request.urlopen(url, context=ctx).read()\\n                        soup = BeautifulSoup(html, 'html.parser')\\n                        con = 0\\n                        tags = soup('a')\\n\\n# Code: http://www.py4e.com/code3/urllinks.py\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 13fb4e8ba6245bcfc77ffa24d25f96afa2c15e51:.ipynb_checkpoints/Untitled-checkpoint.ipynb
    }
   ],
   "source": [
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\"\"\"\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "con = 0\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')# Solicitud de la direccion web\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser') # El analizador de beautiful inicia a procesar\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    con = con + 1\n",
    "    c = con / 3\n",
    "    if c == 1:\n",
    "        print('URL:',tag.get('href', None))\n",
    "        print('Contents:',tag.contents[0])\n",
    "        print(con)\n",
    "        url = tag.get('href', None)\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        con = 0\n",
    "        tags = soup('a')\n",
    "        for tag2 in tags:\n",
    "            con = con + 1\n",
    "            c = con / 3\n",
    "            if c == 1:\n",
    "                print('URL:',tag2.get('href', None))\n",
    "                print('Contents:',tag2.contents[0])\n",
    "                print(con)\n",
    "                url = tag2.get('href', None)\n",
    "                html = urllib.request.urlopen(url, context=ctx).read()\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                con = 0\n",
    "                tags = soup('a')\n",
    "                for tag3 in tags:\n",
    "                    con = con + 1\n",
    "                    c = con / 3\n",
    "                    if c == 1:\n",
    "                        print('URL:',tag3.get('href', None))\n",
    "                        print('Contents:',tag3.contents[0])\n",
    "                        print(con)\n",
    "                        url = tag3.get('href', None)\n",
    "                        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "                        soup = BeautifulSoup(html, 'html.parser')\n",
    "                        con = 0\n",
    "                        tags = soup('a')\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urllinks.py\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - http://py4e-data.dr-chuck.net/known_by_Elivia.html\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Jayme.html\n",
      "Contents: Jayme\n",
      "18\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Ammar.html\n",
      "Contents: Ammar\n",
      "18\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Malachi.html\n",
      "Contents: Malachi\n",
      "18\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Sebastian.html\n",
      "Contents: Sebastian\n",
      "18\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Kashuf.html\n",
      "Contents: Kashuf\n",
      "18\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Rice.html\n",
      "Contents: Rice\n",
      "18\n",
      "URL: http://py4e-data.dr-chuck.net/known_by_Sol.html\n",
      "Contents: Sol\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "con = 0\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')# Solicitud de la direccion web\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser') # El analizador de beautiful inicia a procesar\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    con = con + 1\n",
    "    c = con / 18\n",
    "    if c == 1:\n",
    "        print('URL:',tag.get('href', None))\n",
    "        print('Contents:',tag.contents[0])\n",
    "        print(con)\n",
    "        url = tag.get('href', None)\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        con = 0\n",
    "        tags = soup('a')\n",
    "        break\n",
    "for tag2 in tags:\n",
    "    con = con + 1\n",
    "    c = con / 18\n",
    "    if c == 1:\n",
    "        print('URL:',tag2.get('href', None))\n",
    "        print('Contents:',tag2.contents[0])\n",
    "        print(con)\n",
    "        url = tag2.get('href', None)\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        con = 0\n",
    "        tags = soup('a')\n",
    "        break\n",
    "for tag3 in tags:\n",
    "    con = con + 1\n",
    "    c = con / 18\n",
    "    if c == 1:\n",
    "        print('URL:',tag3.get('href', None))\n",
    "        print('Contents:',tag3.contents[0])\n",
    "        print(con)\n",
    "        url = tag3.get('href', None)\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        con = 0\n",
    "        tags = soup('a')\n",
    "        break\n",
    "for tag4 in tags:\n",
    "    con = con + 1\n",
    "    c = con / 18\n",
    "    if c == 1:\n",
    "        print('URL:',tag4.get('href', None))\n",
    "        print('Contents:',tag4.contents[0])\n",
    "        print(con)\n",
    "        url = tag4.get('href', None)\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        con = 0\n",
    "        tags = soup('a')\n",
    "        break\n",
    "for tag5 in tags:\n",
    "    con = con + 1\n",
    "    c = con / 18\n",
    "    if c == 1:\n",
    "        print('URL:',tag5.get('href', None))\n",
    "        print('Contents:',tag5.contents[0])\n",
    "        print(con)\n",
    "        url = tag5.get('href', None)\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        con = 0\n",
    "        tags = soup('a')\n",
    "        break\n",
    "for tag6 in tags:\n",
    "    con = con + 1\n",
    "    c = con / 18\n",
    "    if c == 1:\n",
    "        print('URL:',tag6.get('href', None))\n",
    "        print('Contents:',tag6.contents[0])\n",
    "        print(con)\n",
    "        url = tag6.get('href', None)\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        con = 0\n",
    "        tags = soup('a')\n",
    "        break\n",
    "for tag7 in tags:\n",
    "    con = con + 1\n",
    "    c = con / 18\n",
    "    if c == 1:\n",
    "        print('URL:',tag7.get('href', None))\n",
    "        print('Contents:',tag7.contents[0])\n",
    "        print(con)\n",
    "        url = tag7.get('href', None)\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        con = 0\n",
    "        tags = soup('a')\n",
    "        break\n",
    "\n",
    "        \n",
    "# Code: http://www.py4e.com/code3/urllinks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas importantes\n",
    "\n",
    "El método <b>encode()</b> devuelve una versión codificada de la cadena. La codificación predeterminada es la codificación de cadena predeterminada actual. Los errores se pueden dar para establecer un esquema de manejo de errores diferente.\n",
    "\n",
    "\n",
    "<b>decode()</b> Decodifica la cadena usando el códec registrado para la codificación.\n",
    "\n",
    "<b>Socket</b> designa un concepto abstracto por el cual dos programas (posiblemente situados en computadoras distintas) pueden intercambiar cualquier flujo de datos, generalmente de manera fiable y ordenada.\n",
    "\n",
    "<b>Bufer</b>En informática, un búfer (del inglés, buffer) es un espacio de memoria, en el que se almacenan datos de manera temporal, normalmente para un único uso (generalmente utilizan un sistema de cola FIFO); su principal uso es para evitar que el programa o recurso que los requiere, ya sea hardware o software, se quede sin datos durante una transferencia (entrada/salida) de datos irregular o por la velocidad del proceso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
